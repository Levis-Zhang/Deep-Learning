# 1 人工智能基础
## 1.1 科普
### 1.1.1 什么是神经网络
黑盒子
像人的大脑🧠一样
任务核心是模型的定义以及模型的参数求解方式，对这两者进行抽象之后，可以确定一个唯一的计算逻辑
## 1.2 基础知识
### 1.2.1 如何将数据输入到神经网络中
转化为特征向量，输入
### 1.2.2 神经网络是如何进行预测的
z = dot(w,x) + b
active function 
     sigmoid
         $\sigma (z)=\frac 1 {1+e^{-z}}$
### 1.2.3 预测得准确吗
验证学习成果
+ 损失函数（loss function)
$$
\begin{aligned}
& \hat{y}^{(i)}=\sigma\left(w^T x^{(i)}+b\right) \\
& \sigma\left(z^{(i)}\right)=\frac{1}{1+e^{-z^{(i)}}}
\end{aligned}
$$
解释
    $\hat y$ 是预测的结果
    $i$ 角标指代某一个训练样本 
    例如
        $\hat{y}^{(i)}$ 是对于训练样本 $x^{(i)}$ 的预测结果
        $L\left(\hat{y}^{(i)}, y^{(i)}\right)=\frac{1}{2}\left(\hat{y}^{(i)}-y^{(i)}\right)^2$
实践中我们使用的损失函数的公式如下。
$$
L\left(\hat{y}^{(i)}, y^{(i)}\right)=-\left(y^{(i)} \log \left(\hat{y}^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-\hat{y}^{(i)}\right)\right)
$$
+ 成本函数（cost function）
    针对整个训练集的损失函数
+ 分布
    原始分布  正确的分布(ground truth)
    目前分布  模型拟合的分布(prediction)
### 1.2.4 网络是如何进行学习的
梯度下降(gradient descent)
学习率$r$
$w’=w-r\mathrm{d}w$
### 1.2.5 计算图
有向无环图
    一个人工智能学习任务的核心是模型的定义以及模型的参数求解方式，对这两者进行抽象之后，可以确定一个唯一的计算逻辑，将这个逻辑用图表示，称之为计算图
前向传播(forward propagation)
反向传播(back propagation)

将最终函数的偏导数的符号进行简化
例如
    将 dJ/dU写成 dU，当你看到 db 时你会知道它是 dJ/db 而不是 dU/db

tensorflow
    用计算图表示计算的人工智能框架，
    数据和计算被转化成计算图的形式
    tensor 数据
    flow 流
    tensorflow 数据流

计算图
    由节点和边组成
    节点
        代表一个操作(tf.Operation)
    边
        代表在节点间传递的张量(tf.Tensor)
### 1.2.6 如何计算逻辑回归的偏导数
逻辑回归(Logistic Regression)
     $\in$广义线性模型(Generalizedliner Model )
     实际是分类模型   常用于二分类
$$偏导数_{多样本}=\overline{偏导数}$$
### 1.2.7 向量化
并行运算  少用显式
去除for循环   增大运行速度
```python
res=np.dot(A,B.T)
```
### 1.2.8 如何开始使用Python
Jupyter notebook
```
Carnet plus 1.2.8
```
### 1.2.9 如何向量化人工智能算法
广播化
     当遇到两个不同维度的对象进行运算，自动复制元素使两个操作对象维度相同
 dot
     矩阵相乘
 *
     元素相乘
```python
Z=np.dot(w.t, X)+b
A=sigma(Z)=1/(1+np.exp(-Z))
J=np.sum(-(Y*np.log (A)+(1-Y)*np.log (1-A)))/m 
dZ=A-Y 
dw=np.dot(X, dZ.t)/m 
db=np.sum(d Z)/m
```
### 1.2.10 一些基础概念
+ 标签
    标签是我们要预测的事物，即简单线性回归中的 $\mathrm{y}$ 变量。
    标签可以是小麦未来的价格、图片中显示的动物品种、音频剪辑的含义或任何事物。
+ 特征
    特征是输入变量, 即简单线性回归中的 $x$ 变量。
    简单的机器学习项目可能会使用单个特征, 而比较复杂的机器学习项目可能会使用数百万个特征。
        在垃圾邮件人工智能检测器中, 特征可能包括：
             电子邮件文本中的字词
             发件人的地址
             发送电子邮件的时段
+ 样本
    样本是指数据的特定实例: $x$ 
    有标签样本
        同时包含特征和标签。即:( $x, y$ )
        使用有标签样本来训练模型
            在垃圾邮件检测器中, 有标签样本是用户明确标记为“垃圾邮件”或“非垃圾邮件”的各个电子邮件。
    无标签样本
        包含特征但不包含标签。即: ( $x$, ?)
        在使用有标签样本训练模型之后, 使用该模型预测无标签样本的标签
         在垃圾邮件检测器中, 无标签样本是用户尚未添加标签的新电子邮件。
模型
    模型定义了特征与标签之间的关系
    例如
        垃圾邮件检测模型可能会将某些特征与“垃圾邮件”紧密联系起来
模型的生命周期
    两个阶段
        训练
            指创建或学习模型
            向模型展示有标签样本, 让模型逐渐学习特征与标签之间的关系
        推断
            指将训练后的模型应用于无标签样本
            使用经过训练的模型做出有用的预测 ( $y^{\prime}$ )。
### 1.2.11 特征工程
将原始数据转化为特征矢量
特征值到数的映射
编码
    独热编码
    多热编码
稀疏表示法
     稀疏向量
     密集向量
 $$稀疏性=\frac {0占的格数}{矩阵总格数}$$
### 1.2.12 哪些特征是有价值的
 避免那些很少出现的特征
 具有清晰明确的含义
 实际数据不要掺入特殊值
     只存储质量评分，不含特殊值
     存储布尔值，表示是否提供quality_rating
### 1.2.13 数据清理
缩放特征值
     Feature Scalling
         最小-最大值归一化
             min-max normalization 
         均值归一化
             mean normalization 
         标准化
             standardization 
清查
### 1.2.14 逻辑回归与分类阈值
+ 线性回归
     linear regression 
     将输入特征进行线性组合输出连续值
+ 逻辑回归
      logistic regression 
      将sigmoid函数用于线性回归(linear regression)
      返回的是概率
+ 分类阈值
     又叫判定阈值
     将逻辑回归值映射到二元类别的值
### 1.2.15 静态训练与动态训练
 

